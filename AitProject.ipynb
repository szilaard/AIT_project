{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/szilaard/AIT_project/blob/main/AitProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EuuKNSnR8YU"
   },
   "source": [
    "# AIT Deep Learning Project\n",
    "\n",
    "Péter Czumbel\n",
    "\n",
    "Szilárd Horváth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5TQ_zjER30L"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import IPython\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the GTZAN dataset from tensorflow datasets doesn't work, the URL times out.<br>\n",
    "See: https://github.com/tensorflow/datasets/issues/4090 <br>\n",
    "Using [this](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification) version of the dataset from kaggle instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all the data from the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob(\"Data/genres_original/*/*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We defined these parameters so we can fine tune them if needed in the future\n",
    "n_fft = 2048\n",
    "n_mfcc = 13\n",
    "hop_length = 512\n",
    "sample_rate = 22050\n",
    "number_of_segments = 5\n",
    "duration = 30\n",
    "samples_per_track = sample_rate * duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the raw data of the first audio sample and its sample frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sr = librosa.load(audio_files[0], sr=sample_rate)\n",
    "print(\"Y is a numpy array:\", signal)\n",
    "print(\"Shape of Y:\", signal.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveshow(signal, sr=sample_rate)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Waveform\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short-time Fourier transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "mfcc = mfcc.T\n",
    "\n",
    "\n",
    "librosa.display.specshow(mfcc, sr=sample_rate, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC\")\n",
    "plt.colorbar(format=\"%+2.0f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"mapping\": [], \n",
    "    \"mfcc\": [],\n",
    "    \"labels\": []\n",
    "}\n",
    "samples_per_segment=int(samples_per_track/number_of_segments)\n",
    "num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next part we separate our audio data into segments, than we use mel frequency cepstral coefficients (MFCCs) on them. This transforms our data closer to what humans would hear/notice listening to the music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_file in audio_files:\n",
    "    label = audio_file.split(\"\\\\\")[1]\n",
    "    if label not in data[\"mapping\"]:\n",
    "        data[\"mapping\"].append(label)\n",
    "    try:\n",
    "        \n",
    "        signal, sr = librosa.load(audio_file)\n",
    "    except:\n",
    "        #there are some corrupted/non readable files so we dont process them\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    for i in range(number_of_segments):\n",
    "        start = samples_per_segment * i\n",
    "        end = start + samples_per_segment\n",
    "        \n",
    "        \n",
    "        if  len(mfcc) == num_mfcc_vectors_per_segment:\n",
    "            mfcc = librosa.feature.mfcc(y=signal[start:end], sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "            mfcc = mfcc.T\n",
    "            data[\"mfcc\"].append(mfcc)\n",
    "            data[\"labels\"].append(data[\"mapping\"].index(label))\n",
    "    \n",
    "    \n",
    "data[\"mapping\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the created lists into numpy arrays, so they are easier to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"mfcc\"] = np.asarray(data[\"mfcc\"])\n",
    "data[\"labels\"]=np.asarray(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate our data into training, validation and test datasets, we define the ratios so we can fine tune them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = len(data[\"mfcc\"])\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_size = int(train_ratio*data_length)\n",
    "valid_size = int(valid_ratio*data_length)\n",
    "test_size = int(test_ratio*data_length)\n",
    "\n",
    "X_train = data[\"mfcc\"][:train_size]\n",
    "Y_train = data[\"labels\"][:train_size]\n",
    "X_valid = data[\"mfcc\"][train_size:train_size+valid_size]\n",
    "Y_valid = data[\"labels\"][train_size:train_size+valid_size]\n",
    "X_test = data[\"mfcc\"][train_size+valid_size:]\n",
    "Y_test = data[\"labels\"][train_size+valid_size:]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPeiS6h3OAXVnkq67zCtqPm",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
